---
title: Fitting study in around work
---

I've been pretty engrossed with my new job, so studying took a back seat for a while. I'm getting back into it now, and I've found several good resources to help me to catch up.


## The firehose of research

My most consistent study method is to listen to podcasts while commuting. Actually mostly YouTube videos; these channels are my staples:

- [Dwarkesh Patel](https://www.youtube.com/@DwarkeshPatel), [80,000 Hours](https://www.youtube.com/@eightythousandhours), and [The AGI Show](https://www.youtube.com/@theagishow)[^theagishow] for deep, long-form interviews with some incredible people right in the thick of it[^longform]
- [Tunadorable](https://www.youtube.com/@Tunadorable) and [AI Coffee Break](https://www.youtube.com/@AICoffeeBreak) for curated paper readings
- [Rob Miles](https://www.youtube.com/@RobertMilesAI) and [Rational Animations](https://www.youtube.com/@RationalAnimations) for my daily dose of doom
- [MattVidProAI](https://www.youtube.com/@MattVidPro) and [Curious Refuge](https://www.youtube.com/@curiousrefuge) to keep one foot on the hype train
- [AI Explained](https://www.youtube.com/@aiexplained-official) and [Sam Witteveen](https://www.youtube.com/@samwitteveenai) for analysis of the rate of capabilities development
- [Andrej Karpathy](https://www.youtube.com/@AndrejKarpathy) and [3Blue1Brown](https://www.youtube.com/@3blue1brown) for deep dives into the code and maths.


## Homework

In terms of actual pen-to-paper _work_, I recently read a summary paper on alignment challenges[^summary]. I marked it up on a paper tablet[^remarkable], and had fun writing up a mock research proposal for addressing one of the challenges.

I've also been messing around with transformers in Colab, seeing what happens when rearranging the architecture a little. It's not my first time writing deep neural network code, but it is the first time I've done more than inference with LLMs. I'd like to share it but I need to consider whether it's an info-hazard. It's pretty small-scale stuff though, so it's probably fine.


[^theagishow]: The AGI Show has ceased, but there are some good interviews in the catalogue.
[^longform]: Never had I listened to a 4.5h interview before, but we live in the future now and it's _weird_ here.
[^summary]: Anwar, Usman, et al. "Foundational challenges in assuring alignment and safety of large language models." _arXiv preprint [arXiv:2404.09932](https://arxiv.org/pdf/2404.09932)_ (2024).
[^remarkable]: It was a borrowed ReMarkable 2, but it may now be my preferred way to read papers. I might have to get one. Not with the default pen though, it's too heavy.
